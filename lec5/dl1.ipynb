{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchaudio import load, transforms\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = transforms.MFCC(sample_rate=8000)\n",
    "\n",
    "def normalize(tensor):\n",
    "    tensor_minusmean = tensor - tensor.mean()\n",
    "    return tensor_minusmean / tensor_minusmean.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WavDataset(Dataset):\n",
    "    def __init__(self, data_folder, length=300000, transform=None):\n",
    "        self.data_folder = data_folder\n",
    "        self.dim = length\n",
    "        self.wav_list = []\n",
    "        self.transform = transform\n",
    "\n",
    "        formats = [\".wav\", \".WAV\"]\n",
    "        for root, dirnames, filenames in os.walk(data_folder):\n",
    "            for filename in filenames:\n",
    "                if os.path.splitext(filename)[1] in formats:\n",
    "                    label = str(root).split(\"/\")[-1]\n",
    "                    self.wav_list.append([os.path.join(root, filename), label])\n",
    "\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        filename, label = self.wav_list[item]\n",
    "        wb_wav, sr = load(filename)\n",
    "        wb_wav = wb_wav[[0], :] # 单声道\n",
    "\n",
    "        length = wb_wav.shape[1]\n",
    "        if length >= self.dim:\n",
    "               max_audio_start = length - self.dim\n",
    "               audio_start = np.random.randint(0, max_audio_start)\n",
    "               wb_wav = wb_wav[audio_start: audio_start + self.dim]\n",
    "        else:\n",
    "            wb_wav = F.pad(wb_wav, (0, self.dim - length), \"constant\", 0)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            wb_wav = normalize(self.transform(wb_wav))\n",
    "            wb_wav = wb_wav.mean()\n",
    "\n",
    "        return wb_wav, sr, filename, label\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.wav_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set= WavDataset(\"datasets/train/\", transform=tf)\n",
    "test_set = WavDataset(\"datasets/test/\", transform=tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[129], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m wavform, sr, name, label \u001b[39m=\u001b[39m train_set[\u001b[39m-\u001b[39;49m\u001b[39m4\u001b[39;49m]\n\u001b[0;32m      2\u001b[0m plt\u001b[39m.\u001b[39mplot(wavform\u001b[39m.\u001b[39mt()\u001b[39m.\u001b[39mnumpy())\n",
      "Cell \u001b[1;32mIn[127], line 31\u001b[0m, in \u001b[0;36mWavDataset.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m     wb_wav \u001b[39m=\u001b[39m normalize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(wb_wav))\n\u001b[1;32m---> 31\u001b[0m     wb_wav \u001b[39m=\u001b[39m mean(wb_wav)\n\u001b[0;32m     33\u001b[0m \u001b[39mreturn\u001b[39;00m wb_wav, sr, filename, label\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mean' is not defined"
     ]
    }
   ],
   "source": [
    "wavform, sr, name, label = train_set[-4]\n",
    "plt.plot(wavform.t().numpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_sample_rate = 8000\n",
    "\n",
    "# transform   = transforms.Resample(orig_freq=sr, new_freq=new_sample_rate)\n",
    "# transformed = transform(wavform)\n",
    "\n",
    "# ipd.Audio(transformed, rate=new_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JSY', 'WX', 'WZH', 'XXL']\n"
     ]
    }
   ],
   "source": [
    "labels = sorted(list(set(datapoint[-1] for datapoint in train_set)))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WX --> tensor(1) --> WX\n"
     ]
    }
   ],
   "source": [
    "def label_to_index(word):\n",
    "    return torch.tensor(labels.index(word))\n",
    "\n",
    "def index_to_label(index):\n",
    "    return labels[index]\n",
    "\n",
    "word_start = \"WX\"\n",
    "index = label_to_index(word_start)\n",
    "word_recovered = index_to_label(index)\n",
    "\n",
    "print(word_start, \"-->\", index, \"-->\", word_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40, 1501])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(batch):\n",
    "    # batch = [item for item in batch] # 将Tensor进行转置\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
    "    return batch.permute(2,1, 0)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # A data tuple has the form:\n",
    "    # wb_wav, sr, filename, label\n",
    "    tensors, targets = [], []\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for waveform, *_, label in batch:\n",
    "        tensors += [waveform]\n",
    "        targets += [label_to_index(label)]\n",
    "\n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = pad_sequence(tensors)\n",
    "    targets = torch.stack(targets)\n",
    "    return tensors, targets\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "if device == \"cuda\":\n",
    "    num_workers = 1\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "number of dims don't match in permute",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[122], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(test_loader))\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "Cell \u001b[1;32mIn[121], line 16\u001b[0m, in \u001b[0;36mcollate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     13\u001b[0m     targets \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [label_to_index(label)]\n\u001b[0;32m     15\u001b[0m \u001b[39m# Group the list of tensors into a batched tensor\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m tensors \u001b[39m=\u001b[39m pad_sequence(tensors)\n\u001b[0;32m     17\u001b[0m targets \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(targets)\n\u001b[0;32m     18\u001b[0m \u001b[39mreturn\u001b[39;00m tensors, targets\n",
      "Cell \u001b[1;32mIn[121], line 4\u001b[0m, in \u001b[0;36mpad_sequence\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpad_sequence\u001b[39m(batch):\n\u001b[0;32m      2\u001b[0m     \u001b[39m# batch = [item for item in batch] # 将Tensor进行转置\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mpad_sequence(batch, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, padding_value\u001b[39m=\u001b[39m\u001b[39m0.\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     \u001b[39mreturn\u001b[39;00m batch\u001b[39m.\u001b[39;49mpermute(\u001b[39m2\u001b[39;49m,\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: number of dims don't match in permute"
     ]
    }
   ],
   "source": [
    "next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "number of dims don't match in permute",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[117], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m X, y \u001b[39min\u001b[39;00m test_loader:\n\u001b[0;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mShape of X [N, C, H, W]: \u001b[39m\u001b[39m\"\u001b[39m, X\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mShape of y: \u001b[39m\u001b[39m\"\u001b[39m, y\u001b[39m.\u001b[39mshape, y\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "Cell \u001b[1;32mIn[115], line 16\u001b[0m, in \u001b[0;36mcollate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     13\u001b[0m     targets \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [label_to_index(label)]\n\u001b[0;32m     15\u001b[0m \u001b[39m# Group the list of tensors into a batched tensor\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m tensors \u001b[39m=\u001b[39m pad_sequence(tensors)\n\u001b[0;32m     17\u001b[0m targets \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(targets)\n\u001b[0;32m     18\u001b[0m \u001b[39mreturn\u001b[39;00m tensors, targets\n",
      "Cell \u001b[1;32mIn[115], line 4\u001b[0m, in \u001b[0;36mpad_sequence\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpad_sequence\u001b[39m(batch):\n\u001b[0;32m      2\u001b[0m     \u001b[39m# batch = [item for item in batch] # 将Tensor进行转置\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mpad_sequence(batch, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, padding_value\u001b[39m=\u001b[39m\u001b[39m0.\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     \u001b[39mreturn\u001b[39;00m batch\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: number of dims don't match in permute"
     ]
    }
   ],
   "source": [
    "for X, y in test_loader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M5(\n",
      "  (conv1): Conv1d(1, 32, kernel_size=(80,), stride=(16,))\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,))\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(32, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n",
      "Number of parameters: 24900\n"
     ]
    }
   ],
   "source": [
    "class M5(nn.Module):\n",
    "    def __init__(self, n_input=1, n_output=35, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.fc1 = nn.Linear(2 * n_channel, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = F.avg_pool1d(x, x.shape[-1])\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=2)\n",
    "\n",
    "model = M5(n_input=transformed.shape[0], n_output=len(labels))\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "n = count_parameters(model)\n",
    "print(\"Number of parameters: %s\" % n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)  # reduce the learning after 20 epochs by a factor of 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "def train(model, epoch, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data   = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        data   = transform(data)\n",
    "        output = model(data)\n",
    "\n",
    "        # 计算 loss\n",
    "        loss = F.nll_loss(output.squeeze(), target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 打印训练进度\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch} Loss: {loss.item():.6f}\")\n",
    "\n",
    "        # 记录 loss\n",
    "        losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算预测正确的数目\n",
    "def number_of_correct(pred, target):\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "# 找到最有可能的标签\n",
    "def get_likely_index(tensor):\n",
    "    return tensor.argmax(dim=-1)\n",
    "\n",
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        data = transform(data)\n",
    "        output = model(data)\n",
    "\n",
    "        pred = get_likely_index(output)\n",
    "        correct += number_of_correct(pred, target)\n",
    "\n",
    "    print(f\"Test Epoch: {epoch} Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Loss: 1.506790\n",
      "Test Epoch: 1 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 2 Loss: 1.129723\n",
      "Test Epoch: 2 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 3 Loss: 0.956235\n",
      "Test Epoch: 3 Accuracy: 4/8 (50%)\n",
      "\n",
      "Train Epoch: 4 Loss: 0.814790\n",
      "Test Epoch: 4 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 5 Loss: 0.701716\n",
      "Test Epoch: 5 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 6 Loss: 0.615186\n",
      "Test Epoch: 6 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 7 Loss: 0.543353\n",
      "Test Epoch: 7 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 8 Loss: 0.469125\n",
      "Test Epoch: 8 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 9 Loss: 0.396290\n",
      "Test Epoch: 9 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 10 Loss: 0.344175\n",
      "Test Epoch: 10 Accuracy: 1/8 (12%)\n",
      "\n",
      "Train Epoch: 11 Loss: 0.296169\n",
      "Test Epoch: 11 Accuracy: 0/8 (0%)\n",
      "\n",
      "Train Epoch: 12 Loss: 0.251269\n",
      "Test Epoch: 12 Accuracy: 0/8 (0%)\n",
      "\n",
      "Train Epoch: 13 Loss: 0.227619\n",
      "Test Epoch: 13 Accuracy: 0/8 (0%)\n",
      "\n",
      "Train Epoch: 14 Loss: 0.182839\n",
      "Test Epoch: 14 Accuracy: 0/8 (0%)\n",
      "\n",
      "Train Epoch: 15 Loss: 0.152129\n",
      "Test Epoch: 15 Accuracy: 0/8 (0%)\n",
      "\n",
      "Train Epoch: 16 Loss: 0.124665\n",
      "Test Epoch: 16 Accuracy: 0/8 (0%)\n",
      "\n",
      "Train Epoch: 17 Loss: 0.097663\n",
      "Test Epoch: 17 Accuracy: 0/8 (0%)\n",
      "\n",
      "Train Epoch: 18 Loss: 0.072337\n",
      "Test Epoch: 18 Accuracy: 0/8 (0%)\n",
      "\n",
      "Train Epoch: 19 Loss: 0.054182\n",
      "Test Epoch: 19 Accuracy: 1/8 (12%)\n",
      "\n",
      "Train Epoch: 20 Loss: 0.041101\n",
      "Test Epoch: 20 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 21 Loss: 0.030540\n",
      "Test Epoch: 21 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 22 Loss: 0.029603\n",
      "Test Epoch: 22 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 23 Loss: 0.028617\n",
      "Test Epoch: 23 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 24 Loss: 0.027622\n",
      "Test Epoch: 24 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 25 Loss: 0.026649\n",
      "Test Epoch: 25 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 26 Loss: 0.025714\n",
      "Test Epoch: 26 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 27 Loss: 0.024816\n",
      "Test Epoch: 27 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 28 Loss: 0.023962\n",
      "Test Epoch: 28 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 29 Loss: 0.023168\n",
      "Test Epoch: 29 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 30 Loss: 0.022394\n",
      "Test Epoch: 30 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 31 Loss: 0.021642\n",
      "Test Epoch: 31 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 32 Loss: 0.020926\n",
      "Test Epoch: 32 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 33 Loss: 0.020254\n",
      "Test Epoch: 33 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 34 Loss: 0.019628\n",
      "Test Epoch: 34 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 35 Loss: 0.019039\n",
      "Test Epoch: 35 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 36 Loss: 0.018481\n",
      "Test Epoch: 36 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 37 Loss: 0.017944\n",
      "Test Epoch: 37 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 38 Loss: 0.017430\n",
      "Test Epoch: 38 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 39 Loss: 0.016935\n",
      "Test Epoch: 39 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 40 Loss: 0.016458\n",
      "Test Epoch: 40 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 41 Loss: 0.016003\n",
      "Test Epoch: 41 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 42 Loss: 0.015958\n",
      "Test Epoch: 42 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 43 Loss: 0.015914\n",
      "Test Epoch: 43 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 44 Loss: 0.015870\n",
      "Test Epoch: 44 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 45 Loss: 0.015826\n",
      "Test Epoch: 45 Accuracy: 2/8 (25%)\n",
      "\n",
      "Train Epoch: 46 Loss: 0.015783\n",
      "Test Epoch: 46 Accuracy: 3/8 (38%)\n",
      "\n",
      "Train Epoch: 47 Loss: 0.015740\n",
      "Test Epoch: 47 Accuracy: 3/8 (38%)\n",
      "\n",
      "Train Epoch: 48 Loss: 0.015697\n",
      "Test Epoch: 48 Accuracy: 3/8 (38%)\n",
      "\n",
      "Train Epoch: 49 Loss: 0.015654\n",
      "Test Epoch: 49 Accuracy: 4/8 (50%)\n",
      "\n",
      "Train Epoch: 50 Loss: 0.015611\n",
      "Test Epoch: 50 Accuracy: 6/8 (75%)\n",
      "\n",
      "Train Epoch: 51 Loss: 0.015569\n",
      "Test Epoch: 51 Accuracy: 6/8 (75%)\n",
      "\n",
      "Train Epoch: 52 Loss: 0.015526\n",
      "Test Epoch: 52 Accuracy: 6/8 (75%)\n",
      "\n",
      "Train Epoch: 53 Loss: 0.015484\n",
      "Test Epoch: 53 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 54 Loss: 0.015442\n",
      "Test Epoch: 54 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 55 Loss: 0.015400\n",
      "Test Epoch: 55 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 56 Loss: 0.015358\n",
      "Test Epoch: 56 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 57 Loss: 0.015317\n",
      "Test Epoch: 57 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 58 Loss: 0.015275\n",
      "Test Epoch: 58 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 59 Loss: 0.015234\n",
      "Test Epoch: 59 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 60 Loss: 0.015192\n",
      "Test Epoch: 60 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 61 Loss: 0.015151\n",
      "Test Epoch: 61 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 62 Loss: 0.015147\n",
      "Test Epoch: 62 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 63 Loss: 0.015143\n",
      "Test Epoch: 63 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 64 Loss: 0.015138\n",
      "Test Epoch: 64 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 65 Loss: 0.015134\n",
      "Test Epoch: 65 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 66 Loss: 0.015130\n",
      "Test Epoch: 66 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 67 Loss: 0.015126\n",
      "Test Epoch: 67 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 68 Loss: 0.015122\n",
      "Test Epoch: 68 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 69 Loss: 0.015118\n",
      "Test Epoch: 69 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 70 Loss: 0.015113\n",
      "Test Epoch: 70 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 71 Loss: 0.015109\n",
      "Test Epoch: 71 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 72 Loss: 0.015105\n",
      "Test Epoch: 72 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 73 Loss: 0.015100\n",
      "Test Epoch: 73 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 74 Loss: 0.015096\n",
      "Test Epoch: 74 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 75 Loss: 0.015092\n",
      "Test Epoch: 75 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 76 Loss: 0.015087\n",
      "Test Epoch: 76 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 77 Loss: 0.015083\n",
      "Test Epoch: 77 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 78 Loss: 0.015079\n",
      "Test Epoch: 78 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 79 Loss: 0.015074\n",
      "Test Epoch: 79 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 80 Loss: 0.015070\n",
      "Test Epoch: 80 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 81 Loss: 0.015065\n",
      "Test Epoch: 81 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 82 Loss: 0.015065\n",
      "Test Epoch: 82 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 83 Loss: 0.015065\n",
      "Test Epoch: 83 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 84 Loss: 0.015064\n",
      "Test Epoch: 84 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 85 Loss: 0.015064\n",
      "Test Epoch: 85 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 86 Loss: 0.015063\n",
      "Test Epoch: 86 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 87 Loss: 0.015063\n",
      "Test Epoch: 87 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 88 Loss: 0.015062\n",
      "Test Epoch: 88 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 89 Loss: 0.015062\n",
      "Test Epoch: 89 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 90 Loss: 0.015061\n",
      "Test Epoch: 90 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 91 Loss: 0.015061\n",
      "Test Epoch: 91 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 92 Loss: 0.015060\n",
      "Test Epoch: 92 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 93 Loss: 0.015060\n",
      "Test Epoch: 93 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 94 Loss: 0.015059\n",
      "Test Epoch: 94 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 95 Loss: 0.015059\n",
      "Test Epoch: 95 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 96 Loss: 0.015059\n",
      "Test Epoch: 96 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 97 Loss: 0.015058\n",
      "Test Epoch: 97 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 98 Loss: 0.015058\n",
      "Test Epoch: 98 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 99 Loss: 0.015057\n",
      "Test Epoch: 99 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 100 Loss: 0.015057\n",
      "Test Epoch: 100 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 101 Loss: 0.015056\n",
      "Test Epoch: 101 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 102 Loss: 0.015056\n",
      "Test Epoch: 102 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 103 Loss: 0.015056\n",
      "Test Epoch: 103 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 104 Loss: 0.015056\n",
      "Test Epoch: 104 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 105 Loss: 0.015056\n",
      "Test Epoch: 105 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 106 Loss: 0.015056\n",
      "Test Epoch: 106 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 107 Loss: 0.015056\n",
      "Test Epoch: 107 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 108 Loss: 0.015056\n",
      "Test Epoch: 108 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 109 Loss: 0.015056\n",
      "Test Epoch: 109 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 110 Loss: 0.015056\n",
      "Test Epoch: 110 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 111 Loss: 0.015056\n",
      "Test Epoch: 111 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 112 Loss: 0.015056\n",
      "Test Epoch: 112 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 113 Loss: 0.015056\n",
      "Test Epoch: 113 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 114 Loss: 0.015056\n",
      "Test Epoch: 114 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 115 Loss: 0.015056\n",
      "Test Epoch: 115 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 116 Loss: 0.015056\n",
      "Test Epoch: 116 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 117 Loss: 0.015055\n",
      "Test Epoch: 117 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 118 Loss: 0.015055\n",
      "Test Epoch: 118 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 119 Loss: 0.015055\n",
      "Test Epoch: 119 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 120 Loss: 0.015055\n",
      "Test Epoch: 120 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 121 Loss: 0.015055\n",
      "Test Epoch: 121 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 122 Loss: 0.015055\n",
      "Test Epoch: 122 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 123 Loss: 0.015055\n",
      "Test Epoch: 123 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 124 Loss: 0.015055\n",
      "Test Epoch: 124 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 125 Loss: 0.015055\n",
      "Test Epoch: 125 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 126 Loss: 0.015055\n",
      "Test Epoch: 126 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 127 Loss: 0.015055\n",
      "Test Epoch: 127 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 128 Loss: 0.015055\n",
      "Test Epoch: 128 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 129 Loss: 0.015055\n",
      "Test Epoch: 129 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 130 Loss: 0.015055\n",
      "Test Epoch: 130 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 131 Loss: 0.015055\n",
      "Test Epoch: 131 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 132 Loss: 0.015055\n",
      "Test Epoch: 132 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 133 Loss: 0.015055\n",
      "Test Epoch: 133 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 134 Loss: 0.015055\n",
      "Test Epoch: 134 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 135 Loss: 0.015055\n",
      "Test Epoch: 135 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 136 Loss: 0.015055\n",
      "Test Epoch: 136 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 137 Loss: 0.015055\n",
      "Test Epoch: 137 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 138 Loss: 0.015055\n",
      "Test Epoch: 138 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 139 Loss: 0.015055\n",
      "Test Epoch: 139 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 140 Loss: 0.015055\n",
      "Test Epoch: 140 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 141 Loss: 0.015055\n",
      "Test Epoch: 141 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 142 Loss: 0.015055\n",
      "Test Epoch: 142 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 143 Loss: 0.015055\n",
      "Test Epoch: 143 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 144 Loss: 0.015055\n",
      "Test Epoch: 144 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 145 Loss: 0.015055\n",
      "Test Epoch: 145 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 146 Loss: 0.015055\n",
      "Test Epoch: 146 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 147 Loss: 0.015055\n",
      "Test Epoch: 147 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 148 Loss: 0.015055\n",
      "Test Epoch: 148 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 149 Loss: 0.015055\n",
      "Test Epoch: 149 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 150 Loss: 0.015055\n",
      "Test Epoch: 150 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 151 Loss: 0.015055\n",
      "Test Epoch: 151 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 152 Loss: 0.015055\n",
      "Test Epoch: 152 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 153 Loss: 0.015055\n",
      "Test Epoch: 153 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 154 Loss: 0.015055\n",
      "Test Epoch: 154 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 155 Loss: 0.015055\n",
      "Test Epoch: 155 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 156 Loss: 0.015055\n",
      "Test Epoch: 156 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 157 Loss: 0.015055\n",
      "Test Epoch: 157 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 158 Loss: 0.015055\n",
      "Test Epoch: 158 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 159 Loss: 0.015055\n",
      "Test Epoch: 159 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 160 Loss: 0.015055\n",
      "Test Epoch: 160 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 161 Loss: 0.015055\n",
      "Test Epoch: 161 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 162 Loss: 0.015055\n",
      "Test Epoch: 162 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 163 Loss: 0.015055\n",
      "Test Epoch: 163 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 164 Loss: 0.015055\n",
      "Test Epoch: 164 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 165 Loss: 0.015055\n",
      "Test Epoch: 165 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 166 Loss: 0.015055\n",
      "Test Epoch: 166 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 167 Loss: 0.015055\n",
      "Test Epoch: 167 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 168 Loss: 0.015055\n",
      "Test Epoch: 168 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 169 Loss: 0.015055\n",
      "Test Epoch: 169 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 170 Loss: 0.015055\n",
      "Test Epoch: 170 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 171 Loss: 0.015055\n",
      "Test Epoch: 171 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 172 Loss: 0.015055\n",
      "Test Epoch: 172 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 173 Loss: 0.015055\n",
      "Test Epoch: 173 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 174 Loss: 0.015055\n",
      "Test Epoch: 174 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 175 Loss: 0.015055\n",
      "Test Epoch: 175 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 176 Loss: 0.015055\n",
      "Test Epoch: 176 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 177 Loss: 0.015055\n",
      "Test Epoch: 177 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 178 Loss: 0.015055\n",
      "Test Epoch: 178 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 179 Loss: 0.015055\n",
      "Test Epoch: 179 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 180 Loss: 0.015055\n",
      "Test Epoch: 180 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 181 Loss: 0.015055\n",
      "Test Epoch: 181 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 182 Loss: 0.015055\n",
      "Test Epoch: 182 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 183 Loss: 0.015055\n",
      "Test Epoch: 183 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 184 Loss: 0.015055\n",
      "Test Epoch: 184 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 185 Loss: 0.015055\n",
      "Test Epoch: 185 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 186 Loss: 0.015055\n",
      "Test Epoch: 186 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 187 Loss: 0.015055\n",
      "Test Epoch: 187 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 188 Loss: 0.015055\n",
      "Test Epoch: 188 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 189 Loss: 0.015055\n",
      "Test Epoch: 189 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 190 Loss: 0.015055\n",
      "Test Epoch: 190 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 191 Loss: 0.015055\n",
      "Test Epoch: 191 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 192 Loss: 0.015055\n",
      "Test Epoch: 192 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 193 Loss: 0.015055\n",
      "Test Epoch: 193 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 194 Loss: 0.015055\n",
      "Test Epoch: 194 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 195 Loss: 0.015055\n",
      "Test Epoch: 195 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 196 Loss: 0.015055\n",
      "Test Epoch: 196 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 197 Loss: 0.015055\n",
      "Test Epoch: 197 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 198 Loss: 0.015055\n",
      "Test Epoch: 198 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 199 Loss: 0.015055\n",
      "Test Epoch: 199 Accuracy: 7/8 (88%)\n",
      "\n",
      "Train Epoch: 200 Loss: 0.015055\n",
      "Test Epoch: 200 Accuracy: 7/8 (88%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_interval = 100  # 每100个batch打印一次训练结果\n",
    "n_epoch = 200\n",
    "\n",
    "# The transform needs to live on the same device as the model and the data.\n",
    "transform = transform.to(device)\n",
    "\n",
    "for epoch in range(1, n_epoch + 1):\n",
    "    train(model, epoch, log_interval)\n",
    "    test(model, epoch)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training Loss')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"Step\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
